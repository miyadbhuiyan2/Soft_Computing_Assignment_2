{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Soft_Computing_Assignment_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score \n",
        "from sklearn.metrics import f1_score \n",
        "from sklearn.metrics import recall_score "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prnDhU3wz0Fb",
        "outputId": "f8b9ac88-b401-4361-dd78-dbee45da56fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing steps"
      ],
      "metadata": {
        "id": "etZsyNn_DRMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('Dataset.csv')\n",
        "df.columns=[\"Text\", \"Sentiment\"]\n",
        "#Removing Stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "df['Text'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "#Stemming\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def stem_sentences(sentence):\n",
        "    tokens = sentence.split()\n",
        "    stemmed_tokens = [ps.stem(token) for token in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "df['Text'] = df['Text'].apply(stem_sentences)\n",
        "df['Sentiment'] = df['Sentiment'].replace(-1,0)"
      ],
      "metadata": {
        "id": "SBgSZaBK5Bsc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF + Neural Network"
      ],
      "metadata": {
        "id": "G6Z95i7VDbJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequences(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.sequences= self.vectorizer.fit_transform(data.Text.tolist())\n",
        "        self.labels=data.Sentiment.tolist()\n",
        "        self.token2idx=self.vectorizer.vocabulary_\n",
        "        self.idx2token={idx: token for token, idx in self.token2idx.items()}\n",
        "    def __getitem__(self, i):\n",
        "        return self.sequences[i, :].toarray(), self.labels[i]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.sequences.shape[0]"
      ],
      "metadata": {
        "id": "KqbNw3eq9EvV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split dataset\n",
        "df_train=df.head(4630) \n",
        "df_test=df.tail(1150) \n",
        "\n",
        "dataset=Sequences(df_train)\n",
        "train_loader=DataLoader(dataset,batch_size=4630)"
      ],
      "metadata": {
        "id": "F3FivLYu8pOZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfIdfNN(nn.Module):\n",
        "  def __init__(self,vocab_size,hidden1,hidden2):\n",
        "    super().__init__()\n",
        "    ###1st hidden\n",
        "    self.linear_1=nn.Linear(vocab_size,hidden1)\n",
        "    self.relu_1=nn.ReLU()\n",
        "\n",
        "    #2nd hidden\n",
        "    self.linear_2=nn.Linear(vocab_size,hidden2)\n",
        "    self.relu_2=nn.ReLU()\n",
        "\n",
        "    #output\n",
        "    self.linear_out=nn.Linear(hidden2,1)\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    #1st hidden\n",
        "    out=self.linear_1(inputs.squeeze(1).float())\n",
        "    out=self.relu_1(out)\n",
        "\n",
        "    #2nd hidden\n",
        "    out=self.linear_2(inputs.squeeze(1).float())\n",
        "    out=self.relu_2(out)\n",
        "\n",
        "    #Linear layout\n",
        "    logits=self.linear_out(out)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "IMkKYnomPEsp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=TfIdfNN(len(dataset.token2idx),100,50)"
      ],
      "metadata": {
        "id": "rZsfPuBhR4Bd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.BCEWithLogitsLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "tf9UulOrSkxL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses=[]\n",
        "\n",
        "for epoch in range(150):\n",
        "  losses=[]\n",
        "  total=0\n",
        "  for inputs,target in train_loader:\n",
        "      model.zero_grad()\n",
        "\n",
        "      output=model(inputs)\n",
        "      loss=criterion(output.squeeze(),target.float())\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      losses.append(loss.item())\n",
        "      total+=1\n",
        "\n",
        "  epoch_loss=sum(losses)/total\n",
        "  train_losses.append(epoch_loss)\n",
        "\n",
        "  print(f'Epoch #{epoch+1}\\tTrain Loss: {epoch_loss:.3f}')\n"
      ],
      "metadata": {
        "id": "aKmh6bMBSzy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5171d35-80bd-4a0e-c9b4-d14365a48864"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1\tTrain Loss: 0.686\n",
            "Epoch #2\tTrain Loss: 0.666\n",
            "Epoch #3\tTrain Loss: 0.643\n",
            "Epoch #4\tTrain Loss: 0.617\n",
            "Epoch #5\tTrain Loss: 0.588\n",
            "Epoch #6\tTrain Loss: 0.559\n",
            "Epoch #7\tTrain Loss: 0.529\n",
            "Epoch #8\tTrain Loss: 0.499\n",
            "Epoch #9\tTrain Loss: 0.470\n",
            "Epoch #10\tTrain Loss: 0.441\n",
            "Epoch #11\tTrain Loss: 0.412\n",
            "Epoch #12\tTrain Loss: 0.384\n",
            "Epoch #13\tTrain Loss: 0.355\n",
            "Epoch #14\tTrain Loss: 0.327\n",
            "Epoch #15\tTrain Loss: 0.300\n",
            "Epoch #16\tTrain Loss: 0.275\n",
            "Epoch #17\tTrain Loss: 0.251\n",
            "Epoch #18\tTrain Loss: 0.230\n",
            "Epoch #19\tTrain Loss: 0.210\n",
            "Epoch #20\tTrain Loss: 0.192\n",
            "Epoch #21\tTrain Loss: 0.174\n",
            "Epoch #22\tTrain Loss: 0.159\n",
            "Epoch #23\tTrain Loss: 0.144\n",
            "Epoch #24\tTrain Loss: 0.131\n",
            "Epoch #25\tTrain Loss: 0.119\n",
            "Epoch #26\tTrain Loss: 0.109\n",
            "Epoch #27\tTrain Loss: 0.100\n",
            "Epoch #28\tTrain Loss: 0.092\n",
            "Epoch #29\tTrain Loss: 0.085\n",
            "Epoch #30\tTrain Loss: 0.078\n",
            "Epoch #31\tTrain Loss: 0.072\n",
            "Epoch #32\tTrain Loss: 0.066\n",
            "Epoch #33\tTrain Loss: 0.061\n",
            "Epoch #34\tTrain Loss: 0.057\n",
            "Epoch #35\tTrain Loss: 0.053\n",
            "Epoch #36\tTrain Loss: 0.049\n",
            "Epoch #37\tTrain Loss: 0.046\n",
            "Epoch #38\tTrain Loss: 0.043\n",
            "Epoch #39\tTrain Loss: 0.040\n",
            "Epoch #40\tTrain Loss: 0.037\n",
            "Epoch #41\tTrain Loss: 0.035\n",
            "Epoch #42\tTrain Loss: 0.033\n",
            "Epoch #43\tTrain Loss: 0.031\n",
            "Epoch #44\tTrain Loss: 0.029\n",
            "Epoch #45\tTrain Loss: 0.027\n",
            "Epoch #46\tTrain Loss: 0.026\n",
            "Epoch #47\tTrain Loss: 0.024\n",
            "Epoch #48\tTrain Loss: 0.023\n",
            "Epoch #49\tTrain Loss: 0.022\n",
            "Epoch #50\tTrain Loss: 0.021\n",
            "Epoch #51\tTrain Loss: 0.020\n",
            "Epoch #52\tTrain Loss: 0.019\n",
            "Epoch #53\tTrain Loss: 0.018\n",
            "Epoch #54\tTrain Loss: 0.017\n",
            "Epoch #55\tTrain Loss: 0.016\n",
            "Epoch #56\tTrain Loss: 0.016\n",
            "Epoch #57\tTrain Loss: 0.015\n",
            "Epoch #58\tTrain Loss: 0.014\n",
            "Epoch #59\tTrain Loss: 0.014\n",
            "Epoch #60\tTrain Loss: 0.013\n",
            "Epoch #61\tTrain Loss: 0.013\n",
            "Epoch #62\tTrain Loss: 0.012\n",
            "Epoch #63\tTrain Loss: 0.012\n",
            "Epoch #64\tTrain Loss: 0.011\n",
            "Epoch #65\tTrain Loss: 0.011\n",
            "Epoch #66\tTrain Loss: 0.010\n",
            "Epoch #67\tTrain Loss: 0.010\n",
            "Epoch #68\tTrain Loss: 0.010\n",
            "Epoch #69\tTrain Loss: 0.009\n",
            "Epoch #70\tTrain Loss: 0.009\n",
            "Epoch #71\tTrain Loss: 0.009\n",
            "Epoch #72\tTrain Loss: 0.009\n",
            "Epoch #73\tTrain Loss: 0.008\n",
            "Epoch #74\tTrain Loss: 0.008\n",
            "Epoch #75\tTrain Loss: 0.008\n",
            "Epoch #76\tTrain Loss: 0.008\n",
            "Epoch #77\tTrain Loss: 0.008\n",
            "Epoch #78\tTrain Loss: 0.007\n",
            "Epoch #79\tTrain Loss: 0.007\n",
            "Epoch #80\tTrain Loss: 0.007\n",
            "Epoch #81\tTrain Loss: 0.007\n",
            "Epoch #82\tTrain Loss: 0.007\n",
            "Epoch #83\tTrain Loss: 0.007\n",
            "Epoch #84\tTrain Loss: 0.006\n",
            "Epoch #85\tTrain Loss: 0.006\n",
            "Epoch #86\tTrain Loss: 0.006\n",
            "Epoch #87\tTrain Loss: 0.006\n",
            "Epoch #88\tTrain Loss: 0.006\n",
            "Epoch #89\tTrain Loss: 0.006\n",
            "Epoch #90\tTrain Loss: 0.006\n",
            "Epoch #91\tTrain Loss: 0.005\n",
            "Epoch #92\tTrain Loss: 0.005\n",
            "Epoch #93\tTrain Loss: 0.005\n",
            "Epoch #94\tTrain Loss: 0.005\n",
            "Epoch #95\tTrain Loss: 0.005\n",
            "Epoch #96\tTrain Loss: 0.005\n",
            "Epoch #97\tTrain Loss: 0.005\n",
            "Epoch #98\tTrain Loss: 0.005\n",
            "Epoch #99\tTrain Loss: 0.005\n",
            "Epoch #100\tTrain Loss: 0.005\n",
            "Epoch #101\tTrain Loss: 0.005\n",
            "Epoch #102\tTrain Loss: 0.004\n",
            "Epoch #103\tTrain Loss: 0.004\n",
            "Epoch #104\tTrain Loss: 0.004\n",
            "Epoch #105\tTrain Loss: 0.004\n",
            "Epoch #106\tTrain Loss: 0.004\n",
            "Epoch #107\tTrain Loss: 0.004\n",
            "Epoch #108\tTrain Loss: 0.004\n",
            "Epoch #109\tTrain Loss: 0.004\n",
            "Epoch #110\tTrain Loss: 0.004\n",
            "Epoch #111\tTrain Loss: 0.004\n",
            "Epoch #112\tTrain Loss: 0.004\n",
            "Epoch #113\tTrain Loss: 0.004\n",
            "Epoch #114\tTrain Loss: 0.004\n",
            "Epoch #115\tTrain Loss: 0.004\n",
            "Epoch #116\tTrain Loss: 0.004\n",
            "Epoch #117\tTrain Loss: 0.004\n",
            "Epoch #118\tTrain Loss: 0.003\n",
            "Epoch #119\tTrain Loss: 0.003\n",
            "Epoch #120\tTrain Loss: 0.003\n",
            "Epoch #121\tTrain Loss: 0.003\n",
            "Epoch #122\tTrain Loss: 0.003\n",
            "Epoch #123\tTrain Loss: 0.003\n",
            "Epoch #124\tTrain Loss: 0.003\n",
            "Epoch #125\tTrain Loss: 0.003\n",
            "Epoch #126\tTrain Loss: 0.003\n",
            "Epoch #127\tTrain Loss: 0.003\n",
            "Epoch #128\tTrain Loss: 0.003\n",
            "Epoch #129\tTrain Loss: 0.003\n",
            "Epoch #130\tTrain Loss: 0.003\n",
            "Epoch #131\tTrain Loss: 0.003\n",
            "Epoch #132\tTrain Loss: 0.003\n",
            "Epoch #133\tTrain Loss: 0.003\n",
            "Epoch #134\tTrain Loss: 0.003\n",
            "Epoch #135\tTrain Loss: 0.003\n",
            "Epoch #136\tTrain Loss: 0.003\n",
            "Epoch #137\tTrain Loss: 0.003\n",
            "Epoch #138\tTrain Loss: 0.003\n",
            "Epoch #139\tTrain Loss: 0.003\n",
            "Epoch #140\tTrain Loss: 0.003\n",
            "Epoch #141\tTrain Loss: 0.003\n",
            "Epoch #142\tTrain Loss: 0.003\n",
            "Epoch #143\tTrain Loss: 0.002\n",
            "Epoch #144\tTrain Loss: 0.002\n",
            "Epoch #145\tTrain Loss: 0.002\n",
            "Epoch #146\tTrain Loss: 0.002\n",
            "Epoch #147\tTrain Loss: 0.002\n",
            "Epoch #148\tTrain Loss: 0.002\n",
            "Epoch #149\tTrain Loss: 0.002\n",
            "Epoch #150\tTrain Loss: 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "  test_vector= torch.LongTensor(dataset.vectorizer.transform([text]).toarray())\n",
        "  output=model(test_vector)\n",
        "  prediction=torch.sigmoid(output).item()\n",
        "\n",
        "  if prediction > 0.5:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "7I3E15e2USpF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels=[]\n",
        "\n",
        "sentences=list(df_test['Text'])\n",
        "labels=df_test['Sentiment']\n",
        "\n",
        "for sentence in sentences:\n",
        "  pred_labels.append(predict_sentiment(sentence))\n",
        "\n",
        "accuracy=accuracy_score(labels,pred_labels)\n",
        "precision=precision_score(labels,pred_labels)\n",
        "recall=recall_score(labels,pred_labels)\n",
        "f1=f1_score(labels,pred_labels)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "print('precision: %f' % precision)\n",
        "print('recall: %f' % recall)\n",
        "print('f1: %f' % f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoPxcGR5reZe",
        "outputId": "c28d8ff7-6f75-4d88-d5d7-00ad139b0e32"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.473913\n",
            "precision: 0.472997\n",
            "recall: 1.000000\n",
            "f1: 0.642224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "result=np.array(list([accuracy,precision,recall,f1]))\n",
        "np.save(\"result.npy\",result)"
      ],
      "metadata": {
        "id": "obufKU4d4D0V"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=np.load(\"result.npy\",allow_pickle=True)\n",
        "ac=result[0]\n",
        "pr=result[1]\n",
        "rec=result[2]\n",
        "f1=result[3]"
      ],
      "metadata": {
        "id": "Cys7LU3j51ko"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMMn1vTp6del",
        "outputId": "7f23b01b-635f-493a-ee18-a807a3bb8b42"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj5EfSeK6FAv",
        "outputId": "3395f621-e717-464d-ef31-76b4836077ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.47391304, 0.47299652, 1.        , 0.64222354])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOW+Neural Network"
      ],
      "metadata": {
        "id": "RGk5assTwg17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequences(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.vectorizer = CountVectorizer()\n",
        "        self.sequences= self.vectorizer.fit_transform(data.Text.tolist())\n",
        "        self.labels=data.Sentiment.tolist()\n",
        "        self.token2idx=self.vectorizer.vocabulary_\n",
        "        self.idx2token={idx: token for token, idx in self.token2idx.items()}\n",
        "    def __getitem__(self, i):\n",
        "        return self.sequences[i, :].toarray(), self.labels[i]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.sequences.shape[0]"
      ],
      "metadata": {
        "id": "0tdwwtLMwl6I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split dataset\n",
        "df_train=df.head(4630) \n",
        "df_test=df.tail(1150)\n",
        "dataset=Sequences(df_train)\n",
        "'''dataset.vectorizer = CountVectorizer()\n",
        "dataset.sequences= dataset.vectorizer.fit_transform(df_train.Text.tolist())\n",
        "dataset.labels=df_train.Sentiment.tolist()\n",
        "dataset.token2idx=dataset.vectorizer.vocabulary_\n",
        "dataset.idx2token={idx: token for token, idx in dataset.token2idx.items()}'''\n",
        "train_loader=DataLoader(dataset,batch_size=4630)"
      ],
      "metadata": {
        "id": "OS0bPGjZFgFB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CVNN(nn.Module):\n",
        "  def __init__(self,vocab_size,hidden1,hidden2):\n",
        "    super().__init__()\n",
        "    ###1st hidden\n",
        "    self.linear_1=nn.Linear(vocab_size,hidden1)\n",
        "    self.relu_1=nn.ReLU()\n",
        "\n",
        "    #2nd hidden\n",
        "    self.linear_2=nn.Linear(vocab_size,hidden2)\n",
        "    self.relu_2=nn.ReLU()\n",
        "\n",
        "    #output\n",
        "    self.linear_out=nn.Linear(hidden2,1)\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    #1st hidden\n",
        "    out=self.linear_1(inputs.squeeze(1).float())\n",
        "    out=self.relu_1(out)\n",
        "\n",
        "    #2nd hidden\n",
        "    out=self.linear_2(inputs.squeeze(1).float())\n",
        "    out=self.relu_2(out)\n",
        "\n",
        "    #Linear layout\n",
        "    logits=self.linear_out(out)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "0wk1LatVFgzO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=CVNN(len(dataset.token2idx),100,50)\n",
        "'''model.linear_1=nn.Linear(len(dataset.token2idx),100)\n",
        "model.relu_1=nn.ReLU()\n",
        "#2nd hidden\n",
        "model.linear_2=nn.Linear(len(dataset.token2idx),50)\n",
        "model.relu_2=nn.ReLU()\n",
        "#output\n",
        "model.linear_out=nn.Linear(50,1)'''"
      ],
      "metadata": {
        "id": "QL3KOYnSFnVP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0f3b624d-9b47-4539-a1b9-c233fffb308f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'model.linear_1=nn.Linear(len(dataset.token2idx),100)\\nmodel.relu_1=nn.ReLU()\\n#2nd hidden\\nmodel.linear_2=nn.Linear(len(dataset.token2idx),50)\\nmodel.relu_2=nn.ReLU()\\n#output\\nmodel.linear_out=nn.Linear(50,1)'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.BCEWithLogitsLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "wO69C_FJGtg8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses=[]\n",
        "\n",
        "for epoch in range(150):\n",
        "  losses=[]\n",
        "  total=0\n",
        "  for inputs,target in train_loader:\n",
        "      model.zero_grad()\n",
        "\n",
        "      output=model(inputs)\n",
        "      loss=criterion(output.squeeze(),target.float())\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      losses.append(loss.item())\n",
        "      total+=1\n",
        "\n",
        "  epoch_loss=sum(losses)/total\n",
        "  train_losses.append(epoch_loss)\n",
        "\n",
        "  print(f'Epoch #{epoch+1}\\tTrain Loss: {epoch_loss:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptIYV9XqF2_t",
        "outputId": "aa138c69-b28d-4ec8-aa10-576e26b03ec5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1\tTrain Loss: 0.692\n",
            "Epoch #2\tTrain Loss: 0.649\n",
            "Epoch #3\tTrain Loss: 0.603\n",
            "Epoch #4\tTrain Loss: 0.557\n",
            "Epoch #5\tTrain Loss: 0.511\n",
            "Epoch #6\tTrain Loss: 0.465\n",
            "Epoch #7\tTrain Loss: 0.419\n",
            "Epoch #8\tTrain Loss: 0.372\n",
            "Epoch #9\tTrain Loss: 0.328\n",
            "Epoch #10\tTrain Loss: 0.289\n",
            "Epoch #11\tTrain Loss: 0.257\n",
            "Epoch #12\tTrain Loss: 0.230\n",
            "Epoch #13\tTrain Loss: 0.205\n",
            "Epoch #14\tTrain Loss: 0.180\n",
            "Epoch #15\tTrain Loss: 0.158\n",
            "Epoch #16\tTrain Loss: 0.139\n",
            "Epoch #17\tTrain Loss: 0.124\n",
            "Epoch #18\tTrain Loss: 0.111\n",
            "Epoch #19\tTrain Loss: 0.099\n",
            "Epoch #20\tTrain Loss: 0.089\n",
            "Epoch #21\tTrain Loss: 0.080\n",
            "Epoch #22\tTrain Loss: 0.072\n",
            "Epoch #23\tTrain Loss: 0.065\n",
            "Epoch #24\tTrain Loss: 0.059\n",
            "Epoch #25\tTrain Loss: 0.054\n",
            "Epoch #26\tTrain Loss: 0.049\n",
            "Epoch #27\tTrain Loss: 0.044\n",
            "Epoch #28\tTrain Loss: 0.040\n",
            "Epoch #29\tTrain Loss: 0.037\n",
            "Epoch #30\tTrain Loss: 0.034\n",
            "Epoch #31\tTrain Loss: 0.031\n",
            "Epoch #32\tTrain Loss: 0.028\n",
            "Epoch #33\tTrain Loss: 0.026\n",
            "Epoch #34\tTrain Loss: 0.024\n",
            "Epoch #35\tTrain Loss: 0.022\n",
            "Epoch #36\tTrain Loss: 0.020\n",
            "Epoch #37\tTrain Loss: 0.019\n",
            "Epoch #38\tTrain Loss: 0.018\n",
            "Epoch #39\tTrain Loss: 0.016\n",
            "Epoch #40\tTrain Loss: 0.015\n",
            "Epoch #41\tTrain Loss: 0.014\n",
            "Epoch #42\tTrain Loss: 0.013\n",
            "Epoch #43\tTrain Loss: 0.013\n",
            "Epoch #44\tTrain Loss: 0.012\n",
            "Epoch #45\tTrain Loss: 0.011\n",
            "Epoch #46\tTrain Loss: 0.010\n",
            "Epoch #47\tTrain Loss: 0.010\n",
            "Epoch #48\tTrain Loss: 0.009\n",
            "Epoch #49\tTrain Loss: 0.009\n",
            "Epoch #50\tTrain Loss: 0.008\n",
            "Epoch #51\tTrain Loss: 0.008\n",
            "Epoch #52\tTrain Loss: 0.008\n",
            "Epoch #53\tTrain Loss: 0.007\n",
            "Epoch #54\tTrain Loss: 0.007\n",
            "Epoch #55\tTrain Loss: 0.007\n",
            "Epoch #56\tTrain Loss: 0.006\n",
            "Epoch #57\tTrain Loss: 0.006\n",
            "Epoch #58\tTrain Loss: 0.006\n",
            "Epoch #59\tTrain Loss: 0.006\n",
            "Epoch #60\tTrain Loss: 0.006\n",
            "Epoch #61\tTrain Loss: 0.005\n",
            "Epoch #62\tTrain Loss: 0.005\n",
            "Epoch #63\tTrain Loss: 0.005\n",
            "Epoch #64\tTrain Loss: 0.005\n",
            "Epoch #65\tTrain Loss: 0.005\n",
            "Epoch #66\tTrain Loss: 0.005\n",
            "Epoch #67\tTrain Loss: 0.004\n",
            "Epoch #68\tTrain Loss: 0.004\n",
            "Epoch #69\tTrain Loss: 0.004\n",
            "Epoch #70\tTrain Loss: 0.004\n",
            "Epoch #71\tTrain Loss: 0.004\n",
            "Epoch #72\tTrain Loss: 0.004\n",
            "Epoch #73\tTrain Loss: 0.004\n",
            "Epoch #74\tTrain Loss: 0.004\n",
            "Epoch #75\tTrain Loss: 0.004\n",
            "Epoch #76\tTrain Loss: 0.004\n",
            "Epoch #77\tTrain Loss: 0.003\n",
            "Epoch #78\tTrain Loss: 0.003\n",
            "Epoch #79\tTrain Loss: 0.003\n",
            "Epoch #80\tTrain Loss: 0.003\n",
            "Epoch #81\tTrain Loss: 0.003\n",
            "Epoch #82\tTrain Loss: 0.003\n",
            "Epoch #83\tTrain Loss: 0.003\n",
            "Epoch #84\tTrain Loss: 0.003\n",
            "Epoch #85\tTrain Loss: 0.003\n",
            "Epoch #86\tTrain Loss: 0.003\n",
            "Epoch #87\tTrain Loss: 0.003\n",
            "Epoch #88\tTrain Loss: 0.003\n",
            "Epoch #89\tTrain Loss: 0.003\n",
            "Epoch #90\tTrain Loss: 0.003\n",
            "Epoch #91\tTrain Loss: 0.003\n",
            "Epoch #92\tTrain Loss: 0.003\n",
            "Epoch #93\tTrain Loss: 0.003\n",
            "Epoch #94\tTrain Loss: 0.003\n",
            "Epoch #95\tTrain Loss: 0.003\n",
            "Epoch #96\tTrain Loss: 0.002\n",
            "Epoch #97\tTrain Loss: 0.002\n",
            "Epoch #98\tTrain Loss: 0.002\n",
            "Epoch #99\tTrain Loss: 0.002\n",
            "Epoch #100\tTrain Loss: 0.002\n",
            "Epoch #101\tTrain Loss: 0.002\n",
            "Epoch #102\tTrain Loss: 0.002\n",
            "Epoch #103\tTrain Loss: 0.002\n",
            "Epoch #104\tTrain Loss: 0.002\n",
            "Epoch #105\tTrain Loss: 0.002\n",
            "Epoch #106\tTrain Loss: 0.002\n",
            "Epoch #107\tTrain Loss: 0.002\n",
            "Epoch #108\tTrain Loss: 0.002\n",
            "Epoch #109\tTrain Loss: 0.002\n",
            "Epoch #110\tTrain Loss: 0.002\n",
            "Epoch #111\tTrain Loss: 0.002\n",
            "Epoch #112\tTrain Loss: 0.002\n",
            "Epoch #113\tTrain Loss: 0.002\n",
            "Epoch #114\tTrain Loss: 0.002\n",
            "Epoch #115\tTrain Loss: 0.002\n",
            "Epoch #116\tTrain Loss: 0.002\n",
            "Epoch #117\tTrain Loss: 0.002\n",
            "Epoch #118\tTrain Loss: 0.002\n",
            "Epoch #119\tTrain Loss: 0.002\n",
            "Epoch #120\tTrain Loss: 0.002\n",
            "Epoch #121\tTrain Loss: 0.002\n",
            "Epoch #122\tTrain Loss: 0.002\n",
            "Epoch #123\tTrain Loss: 0.002\n",
            "Epoch #124\tTrain Loss: 0.002\n",
            "Epoch #125\tTrain Loss: 0.002\n",
            "Epoch #126\tTrain Loss: 0.002\n",
            "Epoch #127\tTrain Loss: 0.002\n",
            "Epoch #128\tTrain Loss: 0.002\n",
            "Epoch #129\tTrain Loss: 0.002\n",
            "Epoch #130\tTrain Loss: 0.002\n",
            "Epoch #131\tTrain Loss: 0.002\n",
            "Epoch #132\tTrain Loss: 0.002\n",
            "Epoch #133\tTrain Loss: 0.002\n",
            "Epoch #134\tTrain Loss: 0.002\n",
            "Epoch #135\tTrain Loss: 0.002\n",
            "Epoch #136\tTrain Loss: 0.002\n",
            "Epoch #137\tTrain Loss: 0.002\n",
            "Epoch #138\tTrain Loss: 0.002\n",
            "Epoch #139\tTrain Loss: 0.002\n",
            "Epoch #140\tTrain Loss: 0.002\n",
            "Epoch #141\tTrain Loss: 0.001\n",
            "Epoch #142\tTrain Loss: 0.001\n",
            "Epoch #143\tTrain Loss: 0.001\n",
            "Epoch #144\tTrain Loss: 0.001\n",
            "Epoch #145\tTrain Loss: 0.001\n",
            "Epoch #146\tTrain Loss: 0.001\n",
            "Epoch #147\tTrain Loss: 0.001\n",
            "Epoch #148\tTrain Loss: 0.001\n",
            "Epoch #149\tTrain Loss: 0.001\n",
            "Epoch #150\tTrain Loss: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "  test_vector= torch.LongTensor(dataset.vectorizer.transform([text]).toarray())\n",
        "  output=model(test_vector)\n",
        "  prediction=torch.sigmoid(output).item()\n",
        "\n",
        "  if prediction > 0.5:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "X9XR3wxQF3KR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pred_labels=[]\n",
        "\n",
        "sentences=list(df_test['Text'])\n",
        "labels=df_test['Sentiment']\n",
        "\n",
        "for sentence in sentences:\n",
        "  pred_labels.append(predict_sentiment(sentence))\n",
        "\n",
        "accuracy2=accuracy_score(labels,pred_labels)\n",
        "precision2=precision_score(labels,pred_labels)\n",
        "recall2=recall_score(labels,pred_labels)\n",
        "f12=f1_score(labels,pred_labels)\n",
        "print('Accuracy: %f' % accuracy2)\n",
        "print('precision: %f' % precision2)\n",
        "print('recall: %f' % recall2)\n",
        "print('f1: %f' % f12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBVS0mxYGKKs",
        "outputId": "23a8f0f6-3ea0-498b-abea-99b7e5062fbf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.577391\n",
            "precision: 0.532646\n",
            "recall: 0.856354\n",
            "f1: 0.656780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x=[\"Neural Network with BoW\",\"Neural Network with Tf-Idf\"]\n",
        "w=0.4\n",
        "Ac=[accuracy2,ac]\n",
        "bar1=np.arange(len(x))\n",
        "plt.bar(bar1,Ac,w,label=\"Accuracy\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xticks(bar1,x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "808cE3_E8lyt",
        "outputId": "35454ef2-0c28-4df0-a42c-263d6e825a78"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<matplotlib.axis.XTick at 0x7f456a2447d0>,\n",
              "  <matplotlib.axis.XTick at 0x7f4567668910>],\n",
              " [Text(0, 0, 'Neural Network with BoW'),\n",
              "  Text(0, 0, 'Neural Network with Tf-Idf')])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results=[]\n",
        "results.append(accuracy)\n",
        "results.append(precision)\n",
        "results.append(recall)\n",
        "results.append(f1)\n",
        "results.append(accuracy2)\n",
        "results.append(precision2)\n",
        "results.append(recall2)\n",
        "results.append(f12)\n",
        "results"
      ],
      "metadata": {
        "id": "ESleOYH2VLDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bYqYh-AA1pjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5c8TuGFS1pmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p8Cusn261pqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iGp8bu-U1puQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Embedding + CNN"
      ],
      "metadata": {
        "id": "tq7JOeaXG6nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequences(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.vectorizer = CountVectorizer(binary=True)\n",
        "        self.sequences= self.vectorizer.fit_transform(data.Text.tolist())\n",
        "        self.labels=data.Sentiment.tolist()\n",
        "        self.token2idx=self.vectorizer.vocabulary_\n",
        "        self.idx2token={idx: token for token, idx in self.token2idx.items()}\n",
        "    def __getitem__(self, i):\n",
        "        return self.sequences[i, :].toarray(), self.labels[i]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.sequences.shape[0]"
      ],
      "metadata": {
        "id": "ID6ps4YTHAbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split dataset\n",
        "df_train=df.head(4630) \n",
        "df_test=df.tail(1150) \n",
        "\n",
        "dataset=Sequences(df_train)\n",
        "train_loader=DataLoader(dataset,batch_size=4630)"
      ],
      "metadata": {
        "id": "khjcy20-KqDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cnn(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(cnn, self).__init__()\n",
        "        self.cnn_layer_1 = nn.Conv1d(in_channels=1, out_channels=256,kernel_size=3, stride=1)\n",
        "        self.cnn_layer_2 = nn.Conv1d(in_channels=256, out_channels=32,kernel_size=3, stride=1)\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.maxpool = nn.MaxPool1d(2)\n",
        "        \n",
        "        self.linear_layer_1 = nn.Linear(32*148*148, 512)  #1200 * 512\n",
        "        self.linear_layer_2 = nn.Linear(512, 128) # 512*128\n",
        "        self.linear_layer_3 = nn.Linear(128, 1)   #128*1\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(.2)\n",
        "        # self.flatten = nn.Flatten()\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.cnn_layer_1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        print(x.shape)\n",
        "        \n",
        "        x = self.cnn_layer_2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        print(x.shape)\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        print(x.shape)\n",
        "        \n",
        "        x = self.linear_layer_1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.linear_layer_2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.linear_layer_3(x)\n",
        "        logits = self.sigmoid(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "26_bjjJkKqG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn(len(dataset.token2idx))"
      ],
      "metadata": {
        "id": "VN6gMx3OKqJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.BCEWithLogitsLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "e5Kbx3NFKyAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses=[]\n",
        "\n",
        "for epoch in range(15):\n",
        "  losses=[]\n",
        "  total=0\n",
        "  for inputs,target in train_loader:\n",
        "      model.zero_grad()\n",
        "      \n",
        "      output=model(inputs.type(torch.FloatTensor))\n",
        "      loss=criterion(output.squeeze(),target.float())\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      losses.append(loss.item())\n",
        "      total+=1\n",
        "\n",
        "  epoch_loss=sum(losses)/total\n",
        "  train_losses.append(epoch_loss)\n",
        "\n",
        "  print(f'Epoch #{epoch+1}\\tTrain Loss: {epoch_loss:.3f}')"
      ],
      "metadata": {
        "id": "zJXhMSlEKyDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testModel(model, loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    size = test.shape[0]\n",
        "\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "      for i in range(test.shape[0]):\n",
        "        x, y = torch.reshape(test[i],(1,3,600,600)), torch.tensor([test_y[i]], dtype=torch.float)\n",
        "    \n",
        "        # Compute prediction error\n",
        "        pred = model(x)[0]\n",
        "        loss += loss_fn(pred, y).item()\n",
        "      \n",
        "    loss /= size\n",
        "    \n",
        "    print(f'Testing Loss: {loss}')"
      ],
      "metadata": {
        "id": "zXhN54EoK3cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    trainModel(model, loss_fn, optimizer)\n",
        "    testModel(model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "aAGHm-Q7U5wq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}